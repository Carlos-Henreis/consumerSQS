# Desenvolvimento com GenAI e Banco de Dados Vetoriais

## Sum√°rio
1. Introdu√ß√£o ao GenAI
    1. O que √© IA generativa e como ela funciona.
    2. Modelos de Linguagem de Grande Porte (LLMs) e como eles diferem de outros modelos de IA.
    3. Considera√ß√µes sobre o uso de GenAI.
    4. Contextualiza√ß√£o.

2. Retrieval Augmented Generation (RAG)
    1. O que √© RAG e por que √© importante.
    2. Embedding e similaridade de texto.

3. Bancos de Dados Vetoriais
    1. O que s√£o bancos de dados vetoriais e como funcionam.
    2. Demonstra√ß√£o Pr√°tica Weaviate

4. Demonstra√ß√£o Pr√°tica
    1. Chatbot especialista em microcr√©dito Itau com RAG e banco de dados vetorial.

## Introdu√ß√£o ao GenAI

### Defini√ß√£o

IA Generativa s√£o sistemas de intelig√™ncia artificial capazes de criar novos conte√∫dos ‚Äî texto, imagem, √°udio ou c√≥digo ‚Äî semelhantes aos produzidos por humanos.

### Como funciona:

* Treinada com grandes volumes de dados.
* Aprende padr√µes e correla√ß√µes.
* Gera novas sa√≠das com base em probabilidades, n√£o em compreens√£o real.

<img src="./images/genai-model-process.svg" alt="Vis√£o geral do GenAI" width="600"/>

### Exemplos:

* üó£Ô∏è ChatGPT (texto)
* üé® DALL-E (imagens)
* üéµ Jukebox (√°udio)

### Aplica√ß√µes:

* Chatbots
* Gera√ß√£o de conte√∫do
* Imagens
* C√≥digo
* S√≠ntese de dados

## Modelos de Linguagem de Grande Porte (LLMs)

### O que s√£o:

Modelos de IA generativa especializados em entender e gerar texto.

### Como funcionam:

* Treinados com enormes conjuntos de textos.
* Produzem a resposta mais prov√°vel com base nos padr√µes aprendidos.
* N√£o ‚Äúentendem‚Äù o texto, apenas predizem a pr√≥xima palavra.

## Considera√ß√µes

### üîç Natureza dos LLMs

* S√£o m√°quinas preditivas de texto, n√£o compreendem o que produzem.
* Baseiam-se em padr√µes estat√≠sticos dos dados de treinamento.
* Podem gerar informa√ß√µes incorretas ou tendenciosas.

### üìö Acesso a Dados

* Treinados com textos da internet, livros e fontes p√∫blicas.
* Os dados podem ser de qualidade question√°vel ou at√© mesmo incorretos.
* N√£o possuem acesso a informa√ß√µes em tempo real ou dados propriet√°rios.
* Ao ser solicitado a fornecer uma resposta relacionada a dados novos ou n√£o presentes no conjunto de treinamento, o LLM pode fornecer uma resposta imprecisa.

<img src="./images/llm-missing-data.png" alt="LLM missing data" width="600"/>

üé≠ Precis√£o e Alucina√ß√µes

* Podem gerar informa√ß√µes falsas ou inventadas (‚Äúalucina√ß√µes‚Äù).
* Exemplo real: advogados apresentaram casos fict√≠cios criados por um LLM.

<img src="./images/confused-llm.png" alt="Confused LLM" width="600"/>

## Contextualiza√ß√£o


### üéØ Por que fornecer contexto:
* O contexto melhora a precis√£o e relev√¢ncia das respostas.
* Ajuda o modelo a ancorar-se em fatos reais, reduzindo alucina√ß√µes.

### üìä Como funciona:

* Inclua dados, relat√≥rios ou informa√ß√µes relevantes na sua pergunta.
* Exemplo: ao pedir o resumo de uma empresa, envie tamb√©m dados financeiros ou de mercado.

### üö´ Limites dos LLMs:

* N√£o t√™m acesso a dados em tempo real ou informa√ß√µes propriet√°rias.
* √â preciso fornecer explicitamente os dados necess√°rios no enunciado.
### Exemplos:
<div style="display: flex; gap: 20px;">
    <img src="./images/prompt-no-context.png" alt="LLM without context" width="500"/>
    <img src="./images/prompt-with-context.png" alt="LLM with context" width="500"/>
</div>

## Retrieval Augmented Generation (RAG)

### üß† O que √©:

RAG (Retrieval-Augmented Generation) combina recupera√ß√£o de informa√ß√µes externas com gera√ß√£o por LLMs, produzindo respostas mais precisas, atualizadas e contextualizadas.

### ‚öôÔ∏è Como funciona:
1. Compreens√£o da consulta ‚Äì o sistema interpreta a pergunta do usu√°rio.
2. Recupera√ß√£o de informa√ß√£o ‚Äì busca dados relevantes em fontes externas (documentos, APIs, grafos).
3. Gera√ß√£o da resposta ‚Äì o LLM usa os dados recuperados para gerar uma resposta fundamentada.

üìö Fontes poss√≠veis:
‚Ä¢ Documentos e relat√≥rios
‚Ä¢ APIs com dados em tempo real
‚Ä¢ Grafos de conhecimento

<img src="./images/llm-rag-process.svg" alt="RAG Process" width="600"/>


## Embedding e similaridade de texto

Um dos desafios do RAG √© entender o que o usu√°rio est√° solicitando e encontrar as informa√ß√µes corretas para passar para o LLM.

### üîç O que s√£o embeddings?

Embeddings s√£o representa√ß√µes vetoriais de texto que capturam o significado sem√¢ntico das palavras. Eles permitem que o sistema compreenda a similaridade entre diferentes textos.

### üìè Como funciona a similaridade de texto?

A similaridade de texto √© medida calculando a dist√¢ncia entre os embeddings de diferentes textos. Textos com significados semelhantes ter√£o embeddings pr√≥ximos no espa√ßo vetorial.

## Arquitetura RAG com Embeddings

<img src="./images/llm-rag-create-vector.svg" alt="RAG Process" width="600"/><br>
<img src="./images/llm-rag-vector-process.svg" alt="RAG Process" width="600"/>

## Pratica com Embeddings e busca semantica (similaridade)

Usando uma biblioteca de embeddings, podemos gerar vetores para cada texto e calcular a similaridade:

Execute os dois comandos abaixo:

```python
import lmstudio as lms
import numpy as np


def cosine_similarity(v1, v2):
    """Calcula a similaridade do cosseno entre dois vetores."""
    v1, v2 = np.array(v1), np.array(v2)

    v1, v2 = v1 / np.linalg.norm(v1), v2 / np.linalg.norm(v2)
    return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))

model = lms.embedding_model("nomic-embed-text-v1.5")

# Exemplos
texto1 = "Microcredito para empreendedores de baixa renda"
texto2 = "A Terra tem o formato geoide"

emb1 = model.embed(texto1)
emb2 = model.embed(texto2)
sim = cosine_similarity(emb1, emb2)
print(f"Similaridade entre '{texto1}' e '{texto2}': {sim:.4f}")
```

## Exemplos

| Faixa de Similaridade | Interpreta√ß√£o                  |
|-----------------------|-------------------------------|
| 0.8 - 1.0             | Significados praticamente iguais       |
| 0.6 - 0.8             | Relacionados, mas diferentes         |
| 0.0 - 0.6             | Sem rela√ß√£o significativa |

### Significados praticamente iguais
- Texto 1: "O sol est√° brilhando e o dia est√° ensolarado"
- Texto 2: "O dia est√° ensolarado e o sol brilha forte."
- Similaridade: 0.9363

### Relacionados, mas diferentes
- Texto 1: "Banco de dados vetoriais para IA"
- Texto 2: "Bancos de dados tradicionais"
- Similaridade: 0.7402

### Sem rela√ß√£o significativa
- Texto 1: "Microcredito para empreendedores de baixa renda"
- Texto 2: "A GenAI evoluiu exponencialmente nos √∫ltimos anos"
- Similaridade: 0.5536


## Bancos de Dados Vetoriais

### Defini√ß√£o:
S√£o bancos de dados projetados para armazenar e buscar vetores ‚Äî representa√ß√µes num√©ricas de textos, imagens, √°udios ou outros dados ‚Äî que capturam significado sem√¢ntico, n√£o apenas igualdade literal.

### Por que usar:
Diferente dos bancos relacionais (que buscam por igualdade exata), bancos vetoriais permitem buscas por similaridade, baseadas em dist√¢ncia entre embeddings (ex: cosseno, Euclidiana).

### Como funcionam

1. Convers√£o em Embeddings
    * O dado (ex: texto) √© convertido em um vetor num√©rico por um modelo de embedding.
    * Exemplo: "gato" ‚Üí [0.12, -0.98, 0.43, ‚Ä¶]

2. Indexa√ß√£o Vetorial
    * Os vetores s√£o armazenados com estruturas otimizadas (ex: HNSW, IVF, PQ) para buscas r√°pidas em alta dimens√£o.

3. Busca por Similaridade
    * Em uma consulta, o texto √© convertido em vetor e o sistema encontra os vetores mais pr√≥ximos ‚Äî ou seja, os dados semanticamente mais similares.

### Exemplos de Bancos Vetoriais

Weaviate, Pinecone, FAISS (Meta), Milvus, Qdrant


## Demonstra√ß√£o Pr√°tica Weaviate

Como armazenar, buscar e ranquear documentos semanticamente usando o banco de dados vetorial Weaviate.

1. Instala√ß√£o do Weaviate (via Docker)

```bash
docker run -d \
  --name weaviate \
  -p 8080:8080 \
  -p 50051:50051 \
  -e QUERY_DEFAULTS_LIMIT=25 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true \
  -e PERSISTENCE_DATA_PATH="/var/lib/weaviate" \
  -e ENABLE_MODULES="" \
  -v weaviate_data:/var/lib/weaviate \
  semitechnologies/weaviate:latest
```

* Acessar em: http://localhost:8080/v1/schema

2. Configura√ß√£o do Esquema

```python
import weaviate

client = weaviate.Client("http://localhost:8080")

client.schema.create_class({
    "class": "Document",
    "properties": [
        {
            "name": "content",
            "dataType": ["text"]
        }
    ]
})
``` 

3. Convers√£o em Embeddings e Indexa√ß√£o

```python
from ollama import Ollama

def embed(text: str):
    response = Ollama.embeddings(model='mxbai-embed-large', prompt=text)
    return response['embedding']

documents = [
    "O microcr√©dito √© uma ferramenta financeira para pequenos empreendedores.",
    "Bancos de dados vetoriais armazenam dados como vetores para buscas sem√¢nticas.",
    "A IA generativa cria conte√∫do novo baseado em padr√µes aprendidos."
]

for doc in documents:
    vector = embed(doc)
    client.data_object.create(
        data_object={"content": doc},
        class_name="Document",
        vector=vector
    )   
```

4. Busca por Similaridade

```python
query = "Como funciona o microcr√©dito?"
query_vector = embed(query)
result = client.query.get("Document", ["content"])\
    .with_near_vector({"vector": query_vector, "certainty": 0.7})\
    .with_limit(3)\
    .do()
for item in result['data']['Get']['Document']:
    print(item['content'])
```
